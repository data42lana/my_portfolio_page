### [Project 1. Image classification using neural networks - Rabbit or Squirrel](https://github.com/data42lana/learning_dl_tools)
---
The project is two Jupyter notebooks created in Google Colaboratory. The objective of it is to gain practical skills in building and training CNNs using **TensorFlow/Keras** and **PyTorch** tools. It deals with a task of binary classification.
The Jupyter notebooks describe the following stages of work: 
1) —Åleaning the used image dataset, viewing some of them with the matplotlib library, transforming images, for example, using augmentation;
2) building and training CNNs;
3) improvement of the built model with the help of normalization and regularization layers and hyperparameter tuning;
4) evaluation of better models on test data;
5) transfer learning with fine-tuning.
The idea was to describe all of the above stages using TensorFlow/Keras, and then rewrite them to PyTorch. 
    
### [Project 2. Solving regression and classification tasks with ML - Abalone](https://github.com/data42lana/learning_ml_tools)
---
The project is a Jupyter notebook. The objective of it is to learn how machine learning tools and algorithms (**scikit-learn**, **XGBoost**, **LightGBM**) work in practice. To do this, a dataset was selected as an example, in which the target (the age of abalone) it can be predicted using both regression and classification algorithms. The Jupyter notebook describes the following stages of work: 
1) exploring and visualizing  the available data with the pandas package and the seaborn and matplotlib libraries;
2) feature transformation and dimensionality reduction using the scikit-learn tools;
3) searching for the best regression and classification models (scikit-learn, XGBoost, LightGBM) and configuring them with search by hyperparameters;
4) evaluating the found best models on test data.
 
### [Project 3. Big Data Analysis with PySpark - WDI](https://github.com/data42lana/learning_big_data)
---
The project is a Jupyter notebook created in Google Colaboratory. The objective of it is to learn tools of the **PySpark SQL** module for working with big data on an example of analysis of World Development Indicators. The project used DataFrames and its methods, built-in functions, window functions, and converting SQL queries to DataFrame. 
